When we divide the data amongst many leaves,
we also have fewer data in each leaf.
Leaves with very few datasets will make predictions
that are quite close to those data actual values,
but they may make very unreliable predictions for
new data (because each prediction is based on only a few).

This is a phenomenon called overfitting,
where a model matches the training data almost perfectly,
but does poorly in validation and other new data. On the flip side,
if we make our tree very shallow, it doesn't divide up the houses into very distinct groups.

At an extreme, if a tree divides houses into only 2 or 4,
each group still has a wide variety of houses.
Resulting predictions may be far off for most houses,
even in the training data (and it will be bad in validation too for the same reason).
When a model fails to capture important distinctions and patterns in the data,
so it performs poorly even in training data, that is called underfitting.